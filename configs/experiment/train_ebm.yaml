# @package _global_

# to execute this experiment run:
# python train.py experiment=train_ebm

defaults:
  - override /data: minerl_processed
  - override /model: energy_model
  - override /callbacks: default
  - override /trainer: gpu #ddp #default
  - override /logger: wandb
#   - override /debug: overfit

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["ebm", "debug"]

seed: 12345
test: False
num_frames: 1 #10 #16


trainer:
  min_epochs: 10
  max_epochs: 100
  gradient_clip_val: 10 #0.5
  #val_check_interval: 100
  limit_val_batches: 1

  log_every_n_steps: 1
  num_sanity_val_steps: 1
  
  overfit_batches: 1
  #precision: bf16-mixed

model:
  K: 60 #100 #50
  step_size: 1 #50 #10 #0.01 #100 #0 #0
  alpha: 0.0 #1
  sigma: 0.01 #05
  buffer_size: 1
  replay_prob: 0.0 #95 #0.95


  optimizer:
    lr: 0.001 #1
  
  net:
    max_frames: ${num_frames}
    attention_mode: math
    model_type: DiT-S/8 #16 #16
    

callbacks:
  early_stopping:
    monitor: "train/loss"
    patience: 10000
    mode: "min"

  model_checkpoint:
    monitor: "train/loss"
    mode: "min"
  

  

data:
  batch_size: 1 #32 #64 
  seq_len: ${num_frames}
  num_workers: 8

logger:
  wandb:
    tags: ${tags}
    group: "mem_ebm"
  aim:
    experiment: "mem_ebm"
