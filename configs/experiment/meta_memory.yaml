# @package _global_

defaults:
  - override /data: memory 
  - override /model: ddpm_image
  - override /callbacks: default
  - override /trainer: gpu 
  #- override /trainer:  ddp 
  - override /logger: wandb
  #- override /paths: default
  #- override /debug: overfit

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["ddpm", "debug", "memory_maze"]

seed: 42 
test: False


trainer:
  inference_mode: False
  min_epochs: 10
  max_epochs: 1000
  #gradient_clip_val: 1 
  val_check_interval: 10 #00 #0 #500 

  check_val_every_n_epoch:  #5
  limit_train_batches: 1
  limit_val_batches: 1
  

  log_every_n_steps: 1
  num_sanity_val_steps: 1
  
  #overfit_batches: 1
  precision: bf16-mixed

#paths:
#  data_dir: /data/cvg/sebastian/memory_maze/memory-maze-9x9 
  
#ckpt_path: logs/train/runs/2025-02-21_22-04-15/checkpoints/last.ckpt  # null

model:
  ckpt_path: logs/train/runs/2025-02-21_22-04-15/checkpoints/last.ckpt  # null
  compile_model:  False #true_without_ddp_optimizer # True
  num_inference_steps: 10
  num_gen_steps: 2
  batch_size: ${data.batch_size}

  ###--- Meta-Learning Config ---###
  meta_learning: True 
  lora_rank: 16
  lora_alpha: 1.0
  inner_lr: 0.0002 #0.0001
  num_inner_steps: 1 #200

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001 #0.0001

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.LinearLR
    _partial_: true
    start_factor: 1 
    end_factor: 0.1
    total_iters: ${trainer.max_epochs}

callbacks:
  early_stopping:
    #monitor: "train/loss"
    #patience: 10000
    #mode: "min"

  model_checkpoint:
    monitor: "train/outer_loss"
    #monitor: "train/loss"
    mode: "min"
  
data:
  batch_size: 1 #32 #1 #32 #32 #64 
  num_workers: 32 #16

logger:
  wandb:
    tags: ${tags}
    group: "ddpm_memory_maze"
  aim:
    experiment: "ddpm_memory_maze"
